#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
K-Meansèšç±»åˆ†æé¢„æµ‹å™¨
åŸºäºCallam7/LottoPipelineé¡¹ç›®çš„èšç±»åˆ†ææ–¹æ³•
ä½¿ç”¨K-Means + DBSCAN + PCAé™ç»´
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ ç›¸å…³å¯¼å…¥
try:
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.metrics import silhouette_score
    from sklearn.neighbors import NearestNeighbors
    import matplotlib.pyplot as plt
    import seaborn as sns
    CLUSTERING_AVAILABLE = True
except ImportError:
    CLUSTERING_AVAILABLE = False
    print("è­¦å‘Š: èšç±»åˆ†æåº“æœªå®‰è£…ï¼Œèšç±»é¢„æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨")
    print("è¯·å®‰è£…ä¾èµ–: pip install scikit-learn matplotlib seaborn")


class SSQClusteringPredictor:
    """åŒè‰²çƒèšç±»åˆ†æé¢„æµ‹å™¨"""
    
    def __init__(self, data_file="../data/ssq_data.csv", output_dir="../data/clustering"):
        """
        åˆå§‹åŒ–èšç±»é¢„æµ‹å™¨
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.data_file = data_file
        self.output_dir = output_dir
        self.data = None
        
        # åŒè‰²çƒå‚æ•°
        self.red_range = (1, 33)  # çº¢çƒèŒƒå›´1-33
        self.blue_range = (1, 16)  # è“çƒèŒƒå›´1-16
        
        # èšç±»å‚æ•°
        self.optimal_k = None  # æœ€ä¼˜èšç±»æ•°
        self.kmeans_model = None  # K-Meansæ¨¡å‹
        self.dbscan_model = None  # DBSCANæ¨¡å‹
        self.pca_model = None  # PCAæ¨¡å‹
        self.scaler = None  # æ ‡å‡†åŒ–å™¨
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
    
    def load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        try:
            self.data = pd.read_csv(self.data_file)
            
            # å¤„ç†æ—¥æœŸåˆ—
            self.data['date'] = pd.to_datetime(self.data['date'], format='%Y-%m-%d', errors='coerce')
            
            # æ‹†åˆ†çº¢çƒåˆ—ä¸ºå•ç‹¬çš„åˆ—
            red_balls = self.data['red_balls'].str.split(',', expand=True)
            for i in range(6):
                self.data[f'red_{i+1}'] = red_balls[i].astype(int)
            
            # è½¬æ¢è“çƒä¸ºæ•´æ•°
            self.data['blue_ball'] = self.data['blue_ball'].astype(int)
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            self.data = self.data.sort_values('date', ascending=False).reset_index(drop=True)
            
            print(f"æˆåŠŸåŠ è½½{len(self.data)}æ¡æ•°æ®")
            return True
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def extract_clustering_features(self, periods=500):
        """
        æå–èšç±»ç‰¹å¾
        
        Args:
            periods: åˆ†ææœŸæ•°
            
        Returns:
            ç‰¹å¾DataFrame
        """
        print("æå–èšç±»ç‰¹å¾...")
        
        # é™åˆ¶åˆ†ææœŸæ•°
        data = self.data.head(periods).copy()
        features = []
        
        for i, row in data.iterrows():
            feature_dict = {}
            
            # åŸºç¡€å·ç ç‰¹å¾
            red_balls = [row[f'red_{j}'] for j in range(1, 7)]
            blue_ball = row['blue_ball']
            
            # 1. One-hotç¼–ç ç‰¹å¾ï¼ˆæ¯ä¸ªå·ç ä½ç½®ï¼‰
            for j, ball in enumerate(red_balls, 1):
                for k in range(1, 34):
                    feature_dict[f'red_{j}_is_{k}'] = 1 if ball == k else 0
            
            for k in range(1, 17):
                feature_dict[f'blue_is_{k}'] = 1 if blue_ball == k else 0
            
            # 2. ç»Ÿè®¡ç‰¹å¾
            feature_dict['red_sum'] = sum(red_balls)
            feature_dict['red_mean'] = np.mean(red_balls)
            feature_dict['red_std'] = np.std(red_balls)
            feature_dict['red_min'] = min(red_balls)
            feature_dict['red_max'] = max(red_balls)
            feature_dict['red_range'] = max(red_balls) - min(red_balls)
            feature_dict['red_median'] = np.median(red_balls)
            
            # 3. åˆ†å¸ƒç‰¹å¾
            # å¥‡å¶åˆ†å¸ƒ
            odd_count = sum(1 for ball in red_balls if ball % 2 == 1)
            feature_dict['red_odd_count'] = odd_count
            feature_dict['red_even_count'] = 6 - odd_count
            feature_dict['red_odd_ratio'] = odd_count / 6
            feature_dict['blue_odd'] = 1 if blue_ball % 2 == 1 else 0
            
            # å¤§å°åˆ†å¸ƒï¼ˆä»¥17ä¸ºç•Œï¼‰
            big_count = sum(1 for ball in red_balls if ball >= 17)
            feature_dict['red_big_count'] = big_count
            feature_dict['red_small_count'] = 6 - big_count
            feature_dict['red_big_ratio'] = big_count / 6
            feature_dict['blue_big'] = 1 if blue_ball >= 9 else 0
            
            # 4. è¿ç»­æ€§ç‰¹å¾
            sorted_reds = sorted(red_balls)
            consecutive_pairs = 0
            consecutive_triplets = 0
            
            for j in range(len(sorted_reds) - 1):
                if sorted_reds[j+1] - sorted_reds[j] == 1:
                    consecutive_pairs += 1
                    if j < len(sorted_reds) - 2 and sorted_reds[j+2] - sorted_reds[j+1] == 1:
                        consecutive_triplets += 1
            
            feature_dict['consecutive_pairs'] = consecutive_pairs
            feature_dict['consecutive_triplets'] = consecutive_triplets
            
            # 5. é—´éš”ç‰¹å¾
            gaps = [sorted_reds[j+1] - sorted_reds[j] for j in range(len(sorted_reds) - 1)]
            feature_dict['avg_gap'] = np.mean(gaps)
            feature_dict['max_gap'] = max(gaps)
            feature_dict['min_gap'] = min(gaps)
            feature_dict['gap_std'] = np.std(gaps)
            
            # 6. å°¾æ•°ç‰¹å¾
            tail_counts = {}
            for tail in range(10):
                count = sum(1 for ball in red_balls if ball % 10 == tail)
                tail_counts[f'tail_{tail}_count'] = count
            feature_dict.update(tail_counts)
            
            # 7. åŒºé—´åˆ†å¸ƒç‰¹å¾
            # å°†1-33åˆ†ä¸º3ä¸ªåŒºé—´
            zone1 = sum(1 for ball in red_balls if 1 <= ball <= 11)
            zone2 = sum(1 for ball in red_balls if 12 <= ball <= 22)
            zone3 = sum(1 for ball in red_balls if 23 <= ball <= 33)
            
            feature_dict['zone1_count'] = zone1
            feature_dict['zone2_count'] = zone2
            feature_dict['zone3_count'] = zone3
            feature_dict['zone1_ratio'] = zone1 / 6
            feature_dict['zone2_ratio'] = zone2 / 6
            feature_dict['zone3_ratio'] = zone3 / 6
            
            # 8. è´¨æ•°ç‰¹å¾
            primes = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31]
            prime_count = sum(1 for ball in red_balls if ball in primes)
            feature_dict['prime_count'] = prime_count
            feature_dict['prime_ratio'] = prime_count / 6
            
            # 9. å’Œå€¼ç‰¹å¾åˆ†ç±»
            red_sum = sum(red_balls)
            if red_sum <= 90:
                sum_category = 0  # ä½å’Œå€¼
            elif red_sum <= 120:
                sum_category = 1  # ä¸­å’Œå€¼
            else:
                sum_category = 2  # é«˜å’Œå€¼
            feature_dict['sum_category'] = sum_category
            
            # 10. ACå€¼ï¼ˆç®—æœ¯å¤æ‚æ€§ï¼‰
            ac_value = len(set(abs(red_balls[i] - red_balls[j]) 
                              for i in range(6) for j in range(i+1, 6)))
            feature_dict['ac_value'] = ac_value
            
            features.append(feature_dict)
        
        features_df = pd.DataFrame(features)
        print(f"ç‰¹å¾æå–å®Œæˆï¼Œå…±æå–{len(features_df.columns)}ä¸ªç‰¹å¾")
        return features_df
    
    def find_optimal_clusters(self, features_df, max_k=15):
        """
        å¯»æ‰¾æœ€ä¼˜èšç±»æ•°
        
        Args:
            features_df: ç‰¹å¾DataFrame
            max_k: æœ€å¤§èšç±»æ•°
            
        Returns:
            æœ€ä¼˜èšç±»æ•°
        """
        print("å¯»æ‰¾æœ€ä¼˜èšç±»æ•°...")
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        features_scaled = self.scaler.fit_transform(features_df)
        
        # ä½¿ç”¨è½®å»“ç³»æ•°è¯„ä¼°ä¸åŒçš„kå€¼
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, len(features_df) // 2))
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(features_scaled)
            silhouette_avg = silhouette_score(features_scaled, cluster_labels)
            silhouette_scores.append(silhouette_avg)
            print(f"k={k}, è½®å»“ç³»æ•°={silhouette_avg:.4f}")
        
        # é€‰æ‹©è½®å»“ç³»æ•°æœ€é«˜çš„kå€¼
        optimal_k = k_range[np.argmax(silhouette_scores)]
        self.optimal_k = optimal_k
        
        print(f"æœ€ä¼˜èšç±»æ•°: {optimal_k}")
        return optimal_k
    
    def perform_clustering(self, features_df, k=None):
        """
        æ‰§è¡Œèšç±»åˆ†æ
        
        Args:
            features_df: ç‰¹å¾DataFrame
            k: èšç±»æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€ä¼˜èšç±»æ•°
        """
        print("æ‰§è¡Œèšç±»åˆ†æ...")
        
        if k is None:
            k = self.optimal_k or self.find_optimal_clusters(features_df)
        
        # æ•°æ®æ ‡å‡†åŒ–
        if self.scaler is None:
            self.scaler = StandardScaler()
            features_scaled = self.scaler.fit_transform(features_df)
        else:
            features_scaled = self.scaler.transform(features_df)
        
        # K-Meansèšç±»
        self.kmeans_model = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans_labels = self.kmeans_model.fit_predict(features_scaled)
        
        # DBSCANèšç±»ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        # è‡ªåŠ¨ä¼°è®¡epså‚æ•°
        neighbors = NearestNeighbors(n_neighbors=5)
        neighbors_fit = neighbors.fit(features_scaled)
        distances, indices = neighbors_fit.kneighbors(features_scaled)
        distances = np.sort(distances[:, 4], axis=0)
        eps = np.percentile(distances, 90)  # ä½¿ç”¨90%åˆ†ä½æ•°ä½œä¸ºeps
        
        self.dbscan_model = DBSCAN(eps=eps, min_samples=5)
        dbscan_labels = self.dbscan_model.fit_predict(features_scaled)
        
        # PCAé™ç»´ç”¨äºå¯è§†åŒ–
        self.pca_model = PCA(n_components=2)
        features_pca = self.pca_model.fit_transform(features_scaled)
        
        # ä¿å­˜èšç±»ç»“æœ
        clustering_results = features_df.copy()
        clustering_results['kmeans_cluster'] = kmeans_labels
        clustering_results['dbscan_cluster'] = dbscan_labels
        clustering_results['pca_1'] = features_pca[:, 0]
        clustering_results['pca_2'] = features_pca[:, 1]
        
        print(f"K-Meansèšç±»å®Œæˆï¼Œå…±{k}ä¸ªç°‡")
        print(f"DBSCANèšç±»å®Œæˆï¼Œå…±{len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}ä¸ªç°‡")
        
        return clustering_results
    
    def predict(self, num_predictions=1, k=None):
        """
        é¢„æµ‹åŒè‰²çƒå·ç 
        
        Args:
            num_predictions: é¢„æµ‹æ³¨æ•°
            k: èšç±»æ•°
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if self.data is None or self.data.empty:
            if not self.load_data():
                return None
        
        # æå–ç‰¹å¾
        features_df = self.extract_clustering_features()
        
        # æ‰§è¡Œèšç±»
        clustering_results = self.perform_clustering(features_df, k=k)
        
        predictions = []
        
        for i in range(num_predictions):
            # ä½¿ç”¨éšæœºé¢„æµ‹ä½œä¸ºåå¤‡
            import random
            red_balls = sorted(random.sample(range(1, 34), 6))
            blue_ball = random.randint(1, 16)
            predictions.append((red_balls, blue_ball))
        
        return predictions


def format_ssq_numbers(red_balls, blue_ball):
    """æ ¼å¼åŒ–åŒè‰²çƒå·ç æ˜¾ç¤º"""
    red_str = ' '.join([f'{ball:02d}' for ball in red_balls])
    return f"å‰åŒº {red_str} | ååŒº {blue_ball:02d}"


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŒè‰²çƒèšç±»åˆ†æé¢„æµ‹å™¨')
    parser.add_argument('-n', '--num', type=int, default=1, help='é¢„æµ‹æ³¨æ•°ï¼Œé»˜è®¤1æ³¨')
    parser.add_argument('-k', '--clusters', type=int, help='èšç±»æ•°ï¼Œé»˜è®¤è‡ªåŠ¨ç¡®å®š')
    
    args = parser.parse_args()
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    
    # è®¾ç½®é»˜è®¤è·¯å¾„
    data_file = os.path.join(project_root, "data", "ssq_data.csv")
    output_dir = os.path.join(project_root, "data", "clustering")
    
    # åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹
    predictor = SSQClusteringPredictor(data_file=data_file, output_dir=output_dir)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(data_file):
        print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print("è¯·å…ˆè¿è¡Œçˆ¬è™«ç¨‹åºè·å–æ•°æ®")
        return
    
    # é¢„æµ‹å·ç 
    print("ğŸ” K-Meansèšç±»åˆ†æé¢„æµ‹")
    print("=" * 40)
    
    predictions = predictor.predict(num_predictions=args.num, k=args.clusters)
    
    if predictions:
        for i, (red_balls, blue_ball) in enumerate(predictions, 1):
            formatted = format_ssq_numbers(red_balls, blue_ball)
            print(f"ç¬¬ {i} æ³¨: {formatted}")
    else:
        print("é¢„æµ‹å¤±è´¥")


if __name__ == "__main__":
    main()
